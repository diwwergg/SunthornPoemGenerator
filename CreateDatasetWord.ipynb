{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import pickle \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from sklearn.model_selection import train_test_split\n",
    "from pythainlp.tokenize import word_tokenize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_txt(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Dataset/TextAfterNormalize.txt'\n",
    "data = read_data_txt(file).split('\\n')\n",
    "data = [line for line in data if len(line) > 1]\n",
    "text = '\\n'.join([line for line in data[:4]])\n",
    "print(\"Sample : \\n\", text)\n",
    "print('Length Line :', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 0\n",
    "########## Word Count ############\n",
    "word_dict_count = {}\n",
    "for i in range(len(data)):\n",
    "    tmp = []\n",
    "    count_slash = 0\n",
    "    for j in data[i].split(' '):\n",
    "        tmp.extend(word_tokenize(j, engine=\"newmm\"))\n",
    "        tmp.append(\"/\")\n",
    "    # print(tmp)\n",
    "    data[i] = tmp[0:-1]\n",
    "    if( len(data[i]) > maxlen):\n",
    "        maxlen = len(data[i])\n",
    "    for word_poem in data[i]:\n",
    "        if(word_poem not in word_dict_count.keys()):\n",
    "            word_dict_count[word_poem] = 1\n",
    "        else:\n",
    "            word_dict_count[word_poem]+=1\n",
    "print(\"Finished Counting\")\n",
    "print(maxlen) # 21\n",
    "print(len(word_dict_count)) # 16144\n",
    "print(word_dict_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx = {'UNK': 0}\n",
    "idx_to_word = {0: 'UNK'}\n",
    "word_count = 1\n",
    "# Create word2idx and idx2word dictionaries\n",
    "for idx, (word, count) in enumerate(word_dict_count.items()):\n",
    "    current_idx = idx + 1 # UNK is first index\n",
    "    if word_dict_count[word] > 3 :\n",
    "        word_to_idx[word] = current_idx\n",
    "        idx_to_word[current_idx] = word\n",
    "        word_count+=1\n",
    "vocab_size = len(idx_to_word)\n",
    "\n",
    "print('Length all word :', len(word_dict_count))\n",
    "print('Length word count < 3 :', len(word_dict_count)-word_count)\n",
    "print(\"Length of word_to_idx: \", len(word_to_idx))\n",
    "print(\"Length of idx_to_word: \", len(idx_to_word))\n",
    "print('word_count :', word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
