{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parallel import DataParallel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import random as rnd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharVocab: \n",
    "    ''' Create a Vocabulary for '''\n",
    "    def __init__(self, type_vocab,pad_token='<PAD>', eos_token='<EOS>', unk_token='<UNK>'): #Initialization of the type of vocabulary\n",
    "        self.type = type_vocab\n",
    "        self.int2char = []\n",
    "        if pad_token !=None:\n",
    "            self.int2char += [pad_token]\n",
    "        if eos_token !=None:\n",
    "            self.int2char += [eos_token]\n",
    "        if unk_token !=None:\n",
    "            self.int2char += [unk_token]\n",
    "        self.char2int = {}\n",
    "        \n",
    "    def __call__(self, text): \n",
    "        chars = set(''.join(text))\n",
    "        self.int2char += list(chars)\n",
    "        self.char2int = {char: ind for ind, char in enumerate(self.int2char)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel1(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, n_layers, drop_rate=0.2):\n",
    "        super(RNNModel1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_size = embedding_size\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.drop_rate = drop_rate\n",
    "        self.char2int = None\n",
    "        self.int2char = None\n",
    "\n",
    "        # Defining the layers\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_layers = nn.ModuleList([nn.LSTM(embedding_size, hidden_dim, dropout=drop_rate, batch_first=True) for _ in range(3)])\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, state):\n",
    "#         embed_seq = self.dropout(self.embedding(x))\n",
    "        for layer in self.rnn_layers:\n",
    "#             rnn_out, state = layer(embed_seq, state)\n",
    "            rnn_out, state = layer(x, state)\n",
    "            embed_seq = self.dropout(rnn_out)\n",
    "\n",
    "        rnn_out = rnn_out.contiguous().view(-1, self.hidden_dim)\n",
    "        logits = self.fc(rnn_out)\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, device, batch_size=1):\n",
    "        return (\n",
    "            torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device),\n",
    "            torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
    "        )\n",
    "\n",
    "    def predict(self, input):\n",
    "        logits, hidden = self.forward(input)\n",
    "        probs = F.softmax(logits)\n",
    "        probs = probs.view(input.size(0), input.size(1), probs.size(1))\n",
    "        return probs, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for RunGenerator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'Model'\n",
    "data_dir = 'Dict'\n",
    "\n",
    "# Files Data\n",
    "char_dict_file = 'char_dict.pkl'\n",
    "input_data_file = 'input_data.pkl'\n",
    "int_dict_file = 'int_dict.pkl'\n",
    "\n",
    "# file Model\n",
    "model_info_file = 'model_info.pth'\n",
    "model_file = 'model.pth'# dict\n",
    "model_run_file = 'model_run.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary from the pickle file\n",
    "char_dict_path = os.path.join(data_dir, char_dict_file)\n",
    "input_data_path = os.path.join(data_dir, input_data_file)\n",
    "int_dict_path = os.path.join(data_dir, int_dict_file)\n",
    "\n",
    "# Load the dictionary from the pickle file\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "char_dict = load_pickle(char_dict_path)\n",
    "input_data = load_pickle(input_data_path)\n",
    "int_dict = load_pickle(int_dict_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set vocabulary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of vocabulary:  74\n",
      "Int to Char:  ['<UNK>', 'ณ', 'ซ', '.', 'ฺ', 'ำ', 'ะ', 'ม', 'ด', 'น', 'จ', 'อ', '้', 'ห', 'บ', 'ฬ', 'ส', 'ํ', 'ฮ', 'ย', 'ิ', '็', 'ฉ', 'ฏ', 'ใ', 'ธ', 'ฦ', 'ไ', 'ู', 'ศ', 'แ', '๊', 'ฝ', 'ผ', '์', 'ล', '่', 'ว', 'ี', 'ฌ', 'ษ', 'ป', ' ', 'ฃ', 'ค', '๋', '-', 'ั', 'ท', 'ื', 'ต', 'ฑ', 'ุ', 'ฎ', 'ฆ', 'ๅ', 'ร', 'พ', 'ญ', 'เ', 'โ', 'ภ', 'ง', 'ฐ', 'ึ', 'ฟ', 'ถ', 'ฯ', 'ช', 'า', 'ก', 'ฤ', 'ฒ', 'ข']\n",
      "Char to Int:  {'<UNK>': 0, 'ณ': 1, 'ซ': 2, '.': 3, 'ฺ': 4, 'ำ': 5, 'ะ': 6, 'ม': 7, 'ด': 8, 'น': 9, 'จ': 10, 'อ': 11, '้': 12, 'ห': 13, 'บ': 14, 'ฬ': 15, 'ส': 16, 'ํ': 17, 'ฮ': 18, 'ย': 19, 'ิ': 20, '็': 21, 'ฉ': 22, 'ฏ': 23, 'ใ': 24, 'ธ': 25, 'ฦ': 26, 'ไ': 27, 'ู': 28, 'ศ': 29, 'แ': 30, '๊': 31, 'ฝ': 32, 'ผ': 33, '์': 34, 'ล': 35, '่': 36, 'ว': 37, 'ี': 38, 'ฌ': 39, 'ษ': 40, 'ป': 41, ' ': 42, 'ฃ': 43, 'ค': 44, '๋': 45, '-': 46, 'ั': 47, 'ท': 48, 'ื': 49, 'ต': 50, 'ฑ': 51, 'ุ': 52, 'ฎ': 53, 'ฆ': 54, 'ๅ': 55, 'ร': 56, 'พ': 57, 'ญ': 58, 'เ': 59, 'โ': 60, 'ภ': 61, 'ง': 62, 'ฐ': 63, 'ึ': 64, 'ฟ': 65, 'ถ': 66, 'ฯ': 67, 'ช': 68, 'า': 69, 'ก': 70, 'ฤ': 71, 'ฒ': 72, 'ข': 73}\n"
     ]
    }
   ],
   "source": [
    "# Set vocap \n",
    "vocab = CharVocab('char',None,None,'<UNK>')\n",
    "vocab.int2char = int_dict\n",
    "vocab.char2int = char_dict\n",
    "\n",
    "print('Length of vocabulary: ', len(vocab.int2char))\n",
    "print('Int to Char: ', vocab.int2char)\n",
    "print('Char to Int: ', vocab.char2int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded Success\n",
      "RNNModel1(\n",
      "  (rnn_layers): ModuleList(\n",
      "    (0-2): 3 x LSTM(74, 256, batch_first=True, dropout=0.2)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=74, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the model's parameters\n",
    "\n",
    "model_info_path = os.path.join(model_dir, model_info_file)\n",
    "with open(model_info_path, 'rb') as f:\n",
    "    model_info = torch.load(f)\n",
    "\n",
    "model = RNNModel1(\n",
    "    vocab_size=model_info['vocab_size'],\n",
    "    embedding_size=model_info['embedding_dim'],\n",
    "    hidden_dim=model_info['hidden_dim'],\n",
    "    n_layers=model_info['n_layers'],\n",
    "    drop_rate=model_info['drop_rate']\n",
    ")\n",
    "# Load the model dict\n",
    "# with open(os.path.join(model_dir, model_file), 'rb') as f:\n",
    "#     model_dict = torch.load(f)\n",
    "\n",
    "with open(os.path.join(model_dir, model_run_file), 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    print('Model loaded Success')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model.load_state_dict(model_dict)\n",
    "# print('Model load state dict success')\n",
    "model.to(device)\n",
    "print(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Function And Input text Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(indices, dict_size):\n",
    "    features = np.eye(dict_size, dtype=np.float32)[indices.flatten()]\n",
    "    features = features.reshape((*indices.shape, dict_size))\n",
    "    return features\n",
    "\n",
    "def encode_text(input_text, vocab, one_hot = False):\n",
    "    output = [vocab.char2int.get(character,0) for character in input_text]\n",
    "    \n",
    "    if one_hot:\n",
    "    # One hot encode every integer of the sequence\n",
    "        dict_size = len(vocab.char2int)\n",
    "        return one_hot_encode(output, dict_size)\n",
    "    else:\n",
    "        return np.array(output)\n",
    "\n",
    "def sample_from_probs(probs, top_n=10):\n",
    "    _, indices = torch.sort(probs)\n",
    "    # set probabilities after top_n to 0\n",
    "    probs[indices.data[:-top_n]] = 0\n",
    "    sampled_index = torch.multinomial(probs, 1)\n",
    "    return sampled_index\n",
    "\n",
    "def predict_probs(model, hidden, character, vocab):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[vocab.char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, model.vocab_size)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character, hidden)\n",
    "\n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    return prob, hidden\n",
    "\n",
    "def format_text(text_predicted, line = 4):\n",
    "    count = 0\n",
    "    text = ''\n",
    "    text_custom = text_predicted.split(' ')\n",
    "    for i in range(len(text_custom)):\n",
    "        if (i+1) % 2 == 0 :\n",
    "            text_custom[i] += '\\n'\n",
    "            count += 1\n",
    "        else :\n",
    "            text_custom[i] += ' '\n",
    "        text += text_custom[i]\n",
    "        if count == line:\n",
    "            break\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input text Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_text(model, device, out_len, vocab, top_n=1, start='หัวใจพี่นั้นเองแหลกสลาย') :\n",
    "    model.eval() # eval mode\n",
    "    \n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Generate the initial hidden state\n",
    "    state = model.init_state(device, 1)\n",
    "    \n",
    "    # Warm up the initial state, predicting on the initial string\n",
    "    for ch in chars:\n",
    "        probs, state = predict_probs(model, state, ch, vocab)\n",
    "        next_index = sample_from_probs(probs, top_n)\n",
    "\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        probs, state = predict_probs(model, state, chars, vocab)\n",
    "        next_index = sample_from_probs(probs, top_n)\n",
    "        chars.append(vocab.int2char[next_index.data[0]])\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "แม้นอกังเกาะในจัน กันดู่น้ำคำปรดบ่นน้ำเหนียบ\n",
      "พอเหมือนเข้าเห็นพระจริจรมเขาขัน ต้องขึ้นพี่ระอุรระเหมือนคลื่นใบกายเปลา\n",
      "ชูนจะแก่วพ้วนสาชปา ช่ารอกชีวาตบาสริต\n",
      "จะวิบคิดไปเท้าสุมแหน สัวหนุจษามอรอมมหมาง\n",
      "\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "text_predicted = generate_from_text(model, device, 300, vocab, 2, 'แม้น')\n",
    "text = format_text(text_predicted)\n",
    "print(text)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "แม้นอยังสมงนายลายลายแจ ชริงจะโยงเหนียบยิ่งมางาล\n",
      "ถายล้นบอกจากจากไหว เจ้าเก๋ยบาวกดางสวงพวปลง\n",
      "ไม่มีสี่พีดี้างกันนารยาม แต่น้ำว่นจำนาทินีรีมระ\n",
      "สน้านท้อนละว่งนำตาไหล สงรับเดือนดังกระพัทธรู้\n",
      "\n",
      "-----------------------------\n",
      "จะคงดำระเงียน แต่ปะลงตายมิ่นหยู่มีหญิงกันดาน\n",
      "ว้านถอรสรุงขรินิศษผลา มตคิดฝ้ายไม่เรือตใจ\n",
      "ระประหนาดปลดมาคนมอขมันเมือ พระทุ่งชุ่นพรางพายพระชาน\n",
      "ยันเมื่อยอดปลุดเหมือนเมื่อแสงวั่งสะอัย ที่จะเชยเหย้าเยือนแกล้งเวนเอา\n",
      "\n",
      "-----------------------------\n",
      "มองงม่านมาจนไม่ แม่รองแตรงาพรำพูพจันทุก\n",
      "จนไม่คู้ายายโด้ยช้างลายลายลานแล หล่อนวักเหวนาวิเหว่าขวาวิวาจ\n",
      "กลับพักตร์อิ่งก็จะพายทางไป นี่แกล้ได้กันสักสบพัง\n",
      "เห็นรั่งประยอบซ้อยเลื่อนเรือนั่น เป็นสนุกคิหลมคำชุมกันตรมับ\n",
      "\n",
      "-----------------------------\n",
      "นางไม่เหมือนกันไร ระยางโอธจ์มนกษามตอมทุก\n",
      "เหมือนเพื่อนเหมือนจรจึงทำลักก็รูล ไห้หนัดหนั่นดตัวกังออกค่อยขนา\n",
      "นาตบากลิ้หมากตะยอกกิยงกำบุก นิยินยิ่งยดอาสมอขมิดหมอน\n",
      "สมระสั่นบางไพรกราบทรายพรายพระชานฯ ถึงยิ่งขีโทรงน์สินดิบริง\n",
      "\n",
      "-----------------------------\n",
      "มืออยรงศรีเหนน ครัดพัยเอาที่ตางพระชนนา\n",
      "ทักวิ่นหยู่ดูจิดมิใคร่หลัน ล้วนจรายใสสบคันเหมือนพุ่มพุลม\n",
      "สู้ฟอกหลางทุกขามรามระกำใน ถึงปากจันตนอด่ามเวกาโวงผล\n",
      "เขานี้างดำรากมันอง พี่พี่นอนกรตะท่าดย\n",
      "\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "texts = ['แม้น', 'จะ', 'มอง', 'นาง', 'มือ']\n",
    "for text in texts:\n",
    "    text_predicted = generate_from_text(model, device, 300, vocab, 2, text)\n",
    "    text = format_text(text_predicted)\n",
    "    print(text)\n",
    "    print('-----------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
